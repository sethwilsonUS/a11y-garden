import { NextRequest, NextResponse } from "next/server";
import { generateAISummary, AVAILABLE_MODELS, DEFAULT_AI_MODEL } from "@/lib/ai-summary";

/**
 * POST /api/ai-summary
 *
 * LOCAL / DEMO MODE ONLY — disabled in production.
 *
 * Lightweight endpoint that runs the same OpenAI analysis the CLI uses.
 * The demo page calls this after a scan completes. When OPENAI_API_KEY
 * isn't set in the local environment the endpoint returns 501 and the
 * demo page gracefully falls back to the "sign up" CTA.
 *
 * In production, AI analysis runs via the Convex action (convex/ai.ts)
 * which uses a hardcoded model and is not user-controllable.
 *
 * Accepts an optional `model` field to let the demo page experiment
 * with different OpenAI models (local mode only).
 */
export async function POST(request: NextRequest) {
  // ---- Production guard -----------------------------------------------------
  // This route is only intended for local dev / demo mode. In production,
  // AI summaries are generated by the Convex action (convex/ai.ts) with a
  // hardcoded model. Blocking this route in production prevents it from being
  // used as an unmetered OpenAI proxy.
  if (process.env.NODE_ENV === "production") {
    return NextResponse.json(
      { error: "This endpoint is not available in production" },
      { status: 404 },
    );
  }

  // Fast-exit when the key isn't configured — this is the expected path
  // for zero-config demo mode.
  if (!process.env.OPENAI_API_KEY) {
    return NextResponse.json(
      { error: "OPENAI_API_KEY is not configured" },
      { status: 501 },
    );
  }

  try {
    const { rawViolations, model, platform } = await request.json();

    if (!rawViolations || typeof rawViolations !== "string") {
      return NextResponse.json(
        { error: "rawViolations (string) is required" },
        { status: 400 },
      );
    }

    // Validate the model if provided — only allow known models
    const validModelIds = AVAILABLE_MODELS.map((m) => m.id);
    const selectedModel =
      model && validModelIds.includes(model) ? model : DEFAULT_AI_MODEL;

    const result = await generateAISummary(
      rawViolations,
      selectedModel,
      typeof platform === "string" ? platform : undefined,
    );

    return NextResponse.json(result);
  } catch (error: unknown) {
    const message =
      error instanceof Error ? error.message : "AI analysis failed";
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
